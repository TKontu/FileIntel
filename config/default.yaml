llm:
  provider: "openai"
  model: "gemma3-12b-awq"
  context_length: 128000
  temperature: 0.1
  rate_limit: 999
  # Task timeout limits (prevents worker exhaustion)
  task_timeout_seconds: 300  # 5 minute soft timeout for LLM tasks
  task_hard_limit_seconds: 360  # 6 minute hard kill timeout
  circuit_breaker:
    failure_threshold: 5
    failure_window: 60
    open_duration: 30
  openai:
    base_url: "http://192.168.0.247:9003/v1"
    embedding_base_url: "http://192.168.0.136:9003/v1"  # Separate embedding server (optional - defaults to base_url if not set)
    api_key: ${OPENAI_API_KEY:-ollama}  # Use environment variable or default to "ollama" for local setups
    rate_limit: 999
  anthropic:
    api_key: ${ANTHROPIC_API_KEY:-}  # Load from environment variable, empty if not set
    rate_limit: 15

rag:
  strategy: "separate" # Can be "merge" or "separate", parameter to select if each embeddng query is sent separately or merged into a single prompt
  embedding_provider: "openai"
  embedding_model: "bge-large-en"
  embedding_max_tokens: 450  # Maximum tokens for embedding input (optimal for 512 token model limit)
  enable_two_tier_chunking: false  # Enable two-tier vector/graph chunking system
  chunking:
    chunk_size: 800  #characters, ~200 tokens (safe conservative limit for 512 token model)
    chunk_overlap: 80
    target_sentences: 3  # Target sentences per Vector RAG chunk (conservative)
    overlap_sentences: 1  # Sentences to overlap between Vector RAG chunks (forward overlap)

graphrag:
  # Microsoft GraphRAG settings - uses same chunks as Vector RAG
  llm_model: "gemma3-12b-awq"  # Match main LLM model
  embedding_model: "bge-large-en"  # Match embedding model
  # NOTE: GraphRAG uses existing Vector RAG chunks, no separate chunking needed
  community_levels: 3  # Good for hierarchical community detection
  max_tokens: 12000  #characters, Sufficient for complex analysis
  # Index storage
  index_base_path: "./graphrag_indices"
  cache:
    enabled: true
    ttl_seconds: 3600
    max_size_mb: 500
    redis_host: "redis"
    redis_port: 6379
    redis_db: 0
  # Automatic indexing after vector RAG completion
  auto_index_after_upload: true  # NEW: Enable background GraphRAG indexing
  auto_index_delay_seconds: 30   # NEW: Wait time after vector indexing completes
  # Query routing
  query_classification_model: "gemma3-12b-awq"
  hybrid_query_threshold: 0.7
  # Fallback keyword classification
  graph_keywords:
    - "relationship"
    - "connection"
    - "entity"
    - "community"
    - "network"
    - "how are"
    - "related to"
  vector_keywords:
    - "what is"
    - "define"
    - "explain"
    - "summarize"
    - "list facts about"
  # Async processing for RTX 3090 24GB optimization
  async_processing:
    enabled: true                    # ON/OFF switch (set to false to disable)
    batch_size: 4                   # Concurrent requests per batch
    max_concurrent_requests: 25     # Must match vLLM VLLM_MAX_NUM_SEQS (currently 16)
    batch_timeout: 30               # Timeout per batch (seconds)
    fallback_to_sequential: true    # Fallback if batching fails
  # Embedding configuration
  embedding_batch_max_tokens: 450   # Max tokens per embedding request (optimal for 512 token model limit)

document_processing:
  chunk_size: 800  # ~200 tokens (matches RAG chunk size for consistency)
  overlap: 80
  max_file_size: "300MB"
  supported_formats: ["pdf", "epub", "mobi"]

  # PDF processor selection: "mineru" or "traditional" (auto-fallback on failure)
  primary_pdf_processor: "mineru"

  # Type-aware chunking (Phase 1 - NEW)
  # When enabled: uses element-based chunking with corruption filtering, statistical heuristics, and specialized chunkers
  # When disabled: uses traditional text-based chunking (backward compatible)
  # Default: false (can enable after testing)
  use_type_aware_chunking: true

  # MinerU OCR settings
  mineru:
    # API type: "selfhosted" for FastAPI or "commercial" for async task API
    api_type: "selfhosted"

    # Common settings for both API types
    base_url: "http://192.168.0.136:8000"
    timeout: 600
    enable_formula: false
    enable_table: true
    language: "en"

    # Backend selection for selfhosted API / model version for commercial API
    # - "pipeline": OCR + Layout detection pipeline (faster, 2-10s per document)
    # - "vlm": Vision-Language Model (better for complex layouts, first request slow 30-60s)
    # Note: When using selfhosted API, ensure Docker Compose profile matches this setting:
    #   - model_version: "pipeline" → docker compose --profile pipeline up -d
    #   - model_version: "vlm" → docker compose --profile vlm up -d
    model_version: "pipeline"

    # Debug output settings (for troubleshooting extraction issues)
    # When enabled, saves MinerU raw outputs (markdown, JSON, images) to disk
    # WARNING: Can consume significant disk space - only enable for debugging
    save_outputs: true
    output_directory: "/home/appuser/app/mineru_outputs"

    # Element-level type preservation (EXPERIMENTAL - Phase 1 of structure utilization)
    # When true: preserves MinerU element boundaries (one TextElement per content_list item)
    # When false: concatenates all elements per page (current behavior, backward compatible)
    # Default: false (safe, preserves existing behavior)
    use_element_level_types: false

    # Element filtering (EXPERIMENTAL - Phase 2, requires use_element_level_types=true)
    # When true: filters out TOC/LOF elements before chunking (prevents oversized chunks)
    # When false: all elements pass through to chunking (backward compatible)
    # Default: false (safe, preserves existing behavior)
    enable_element_filtering: false

    # Commercial API specific settings (used when api_type="commercial")
    api_token: ${MINERU_API_TOKEN:-}
    poll_interval: 10
    max_retries: 3
    shared_folder_path: "/shared/uploads"
    shared_folder_url_prefix: "file:///shared/uploads"

ocr:
  primary_engine: "pdf_extract_kit"
  fallback_engines: ["tesseract", "google_vision"]

output:
  default_format: "json"
  output_directory: "./results"
  include_metadata: true
  max_concurrent_jobs: 5

logging:
  level: "INFO"

paths:
  uploads: ${UPLOADS_DIR:-/home/appuser/app/uploads}
  prompts: ${PROMPTS_DIR:-/home/appuser/app/prompts}
  input: ${INPUT_DIR:-/home/appuser/app/input}
  output: ${OUTPUT_DIR:-/home/appuser/app/output}

prompts:
  directory: "${PROMPTS_DIR:-/home/appuser/app/prompts}/templates"

api:
  host: "0.0.0.0"
  port: 8000
  cors_origins: ["http://localhost:3000", "http://localhost:8080", "http://127.0.0.1:3000", "http://127.0.0.1:8080"]  # Restrict to local development by default
  # cors_origins: ["*"]  # Uncomment this line and comment the above for unrestricted access
  rate_limit: 9999
  authentication:
    enabled: false

    api_key: ${API_KEY:-}

storage:
  type: "postgres"
  connection_string: "postgresql://${DB_USER:-user}:${DB_PASSWORD:-password}@${DB_HOST:-localhost}:${DB_PORT:-5432}/${DB_NAME:-fileintel}"
  cache_ttl: 3600
  redis_host: "localhost"
  # Connection pool settings (prevents exhaustion)
  pool_size: 20  # Base pool size
  max_overflow: 30  # Additional connections allowed
  pool_timeout: 30  # Seconds to wait for connection

batch_processing:
  directory_input: ${INPUT_DIR:-/home/appuser/app/input}
  directory_output: ${OUTPUT_DIR:-/home/appuser/app/output}
  default_format: "json"
  # Batch size limits (security - prevent DoS)
  max_upload_batch_size: 50  # Maximum files per batch upload
  max_file_size_mb: 100       # Maximum individual file size in MB
  max_processing_batch_size: 20  # Maximum collections per batch process
