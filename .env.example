# ============================================================================
# FileIntel Environment Configuration
# ============================================================================
# All settings have sensible defaults in config/default.yaml
# Only override the values you need to change
# ============================================================================

# ----------------------------------------------------------------------------
# Database Configuration (PostgreSQL)
# ----------------------------------------------------------------------------
DB_USER=user
DB_PASSWORD=password
DB_HOST=localhost
DB_PORT=5432
DB_NAME=fileintel

# Legacy database variables (deprecated, use DB_* instead)
POSTGRES_USER=user
POSTGRES_PASSWORD=password
POSTGRES_DB=fileintel

# ----------------------------------------------------------------------------
# Storage Configuration
# ----------------------------------------------------------------------------
# STORAGE_TYPE=postgres
# STORAGE_CACHE_TTL=3600
# STORAGE_REDIS_HOST=localhost
# Connection pools: base=10, overflow=20 per worker
# With 6 workers: 60 base, 180 max burst (requires PostgreSQL max_connections≥200)
# STORAGE_POOL_SIZE=10
# STORAGE_MAX_OVERFLOW=20
# STORAGE_POOL_TIMEOUT=300

# ----------------------------------------------------------------------------
# Redis Configuration (for caching)
# ----------------------------------------------------------------------------
# REDIS_HOST=redis
# REDIS_PORT=6379
# REDIS_DB=0

# ----------------------------------------------------------------------------
# Shared Cache Configuration (for Vector RAG and GraphRAG)
# ----------------------------------------------------------------------------
# Single Redis instance used by both SimpleCache (LLM responses) and
# GraphRAGDataFrameCache (parquet DataFrames)
# CACHE_ENABLED=true
# CACHE_TTL=3600
# CACHE_MAX_SIZE_MB=500
# CACHE_REDIS_HOST=redis  # Defaults to REDIS_HOST if not set
# CACHE_REDIS_PORT=6379   # Defaults to REDIS_PORT if not set
# CACHE_REDIS_DB=0        # Defaults to REDIS_DB if not set

# ----------------------------------------------------------------------------
# Logging Configuration
# ----------------------------------------------------------------------------
# LOG_LEVEL=WARNING  # Options: DEBUG, INFO, WARNING, ERROR
# Recommended: WARNING for production, INFO for development, DEBUG for troubleshooting

# Suppress transformers library warnings about missing ML frameworks
# (We only use transformers for tokenization, not for running models)
TRANSFORMERS_NO_ADVISORY_WARNINGS=1

# ----------------------------------------------------------------------------
# LLM Provider Configuration
# ----------------------------------------------------------------------------
# LLM_PROVIDER=openai  # Options: openai, anthropic
# LLM_MODEL=gemma3-12b-awq
# LLM_CONTEXT_LENGTH=128000
# LLM_TEMPERATURE=0.1
# LLM_RATE_LIMIT=999
# LLM_TASK_TIMEOUT=7200
# LLM_TASK_HARD_LIMIT=7200

# HTTP timeout and retry settings (critical for high server load)
# Increase these if your vLLM server has high queue depths (80+ requests)
# LLM_HTTP_TIMEOUT=900  # HTTP request timeout in seconds (default: 15 min, handles queue delays)
# LLM_MAX_RETRIES=5  # Maximum retry attempts on timeout/connection errors
# LLM_RETRY_BACKOFF_MIN=2  # Minimum retry backoff in seconds
# LLM_RETRY_BACKOFF_MAX=60  # Maximum retry backoff in seconds (exponential backoff)

# Circuit breaker settings (legacy, not currently used in code)
# LLM_CIRCUIT_BREAKER_THRESHOLD=5
# LLM_CIRCUIT_BREAKER_WINDOW=60
# LLM_CIRCUIT_BREAKER_DURATION=30

# ----------------------------------------------------------------------------
# OpenAI Configuration
# ----------------------------------------------------------------------------
OPENAI_API_KEY=your_openai_api_key_here
# OPENAI_BASE_URL=http://192.168.0.247:9003/v1
# OPENAI_EMBEDDING_BASE_URL=http://192.168.0.136:9003/v1
# OPENAI_RATE_LIMIT=999

# ----------------------------------------------------------------------------
# Anthropic Configuration
# ----------------------------------------------------------------------------
# ANTHROPIC_API_KEY=your_anthropic_api_key_here
# ANTHROPIC_RATE_LIMIT=15

# ----------------------------------------------------------------------------
# RAG Configuration
# ----------------------------------------------------------------------------
# RAG_STRATEGY=separate  # Options: merge, separate
# RAG_EMBEDDING_PROVIDER=openai
# RAG_EMBEDDING_MODEL=bge-large-en
# RAG_EMBEDDING_MAX_TOKENS=450
# RAG_ENABLE_TWO_TIER_CHUNKING=false

# RAG Chunking
# RAG_CHUNK_SIZE=800
# RAG_CHUNK_OVERLAP=80
# RAG_TARGET_SENTENCES=3
# RAG_OVERLAP_SENTENCES=1

# RAG Embedding Batch Processing (improves throughput 10-25x)
# Batching reduces overhead dramatically: 1000 chunks = 40 tasks instead of 1000
# RAG_EMBEDDING_BATCH_SIZE=25  # Chunks per batch (1=disable, 25=optimal, 50-100=max throughput)
# RAG_EMBEDDING_FALLBACK_SINGLE=true  # Fall back to single-chunk processing on batch failure
# RAG_EMBEDDING_RETRY_INDIVIDUAL=true  # Retry failed chunks individually after batch completes

# RAG Async Processing
# RAG_ASYNC_ENABLED=true
# RAG_ASYNC_BATCH_SIZE=4
# RAG_ASYNC_MAX_CONCURRENT=25
# RAG_ASYNC_BATCH_TIMEOUT=900  # 15 minutes batch timeout (handles high queue depths)
# RAG_ASYNC_FALLBACK=true

# RAG Query Classification (LLM-based intelligent routing)
# RAG_CLASSIFICATION_METHOD=hybrid  # Options: llm (LLM only), keyword (keywords only), hybrid (LLM + keyword fallback)
# RAG_CLASSIFICATION_MODEL=gemma3-4B  # Use small/fast model for classification
# RAG_CLASSIFICATION_TEMPERATURE=0.0  # 0.0 = deterministic classification
# RAG_CLASSIFICATION_MAX_TOKENS=150
# RAG_CLASSIFICATION_TIMEOUT=5  # Seconds before falling back to keywords
# RAG_CLASSIFICATION_CACHE_ENABLED=true  # Cache classification results to reduce costs
# RAG_CLASSIFICATION_CACHE_TTL=3600  # Cache for 1 hour

# RAG Result Reranking (improves relevance, adds 50-200ms latency)
# RAG_RERANKING_ENABLED=false  # Enable to improve result quality

# Reranking API settings (vLLM or OpenAI-compatible server)
# RAG_RERANKING_BASE_URL=http://192.168.0.136:9003/v1  # Reranking server URL
# RAG_RERANKING_API_KEY=ollama  # API key for authentication
# RAG_RERANKING_TIMEOUT=120  # HTTP timeout in seconds (handles cold model loading)

# Model configuration
# RAG_RERANKING_MODEL=BAAI/bge-reranker-v2-m3  # Multilingual reranker model

# Retrieval strategy
# RAG_RERANKING_INITIAL_K=20  # Retrieve more chunks initially
# RAG_RERANKING_FINAL_K=5  # Return top K after reranking

# Strategy - which results to rerank
# RAG_RERANK_VECTOR=true  # Rerank vector search results
# RAG_RERANK_GRAPH=true  # Rerank GraphRAG results
# RAG_RERANK_HYBRID=true  # Rerank hybrid query results

# Optional filtering
# RAG_RERANKING_MIN_SCORE=null  # Filter results below this score (e.g., 0.3)

# ----------------------------------------------------------------------------
# GraphRAG Configuration
# ----------------------------------------------------------------------------
# GRAPHRAG_LLM_MODEL=gemma3-12b-awq
# GRAPHRAG_EMBEDDING_MODEL=bge-large-en
# GRAPHRAG_COMMUNITY_LEVELS=3
# GRAPHRAG_MAX_CLUSTER_SIZE=150  # Leiden algorithm max cluster size (higher = larger clusters)
# GRAPHRAG_LEIDEN_RESOLUTION=0.5  # Leiden resolution parameter (lower = larger communities, try 0.5-0.7 for consolidated hierarchy)
# GRAPHRAG_MAX_TOKENS=12000
# GRAPHRAG_INDEX_PATH=./graphrag_indices

# GraphRAG Query Routing
# GRAPHRAG_CLASSIFICATION_MODEL=gemma3-12b-awq

# GraphRAG Async Processing
# GRAPHRAG_ASYNC_ENABLED=true
# GRAPHRAG_ASYNC_BATCH_SIZE=4
# GRAPHRAG_ASYNC_MAX_CONCURRENT=50
# GRAPHRAG_ASYNC_BATCH_TIMEOUT=900  # 15 minutes batch timeout (handles high queue depths)
# GRAPHRAG_ASYNC_FALLBACK=true
# GRAPHRAG_EMBEDDING_BATCH_MAX_TOKENS=450

# GraphRAG Retry/Timeout Settings (critical for unstable/high-load vLLM servers)
# GRAPHRAG_REQUEST_TIMEOUT=900  # 15 min request timeout (default: 900s)
# GRAPHRAG_MAX_RETRIES=10  # Max retry attempts for 503/timeout errors (default: 10)
# GRAPHRAG_MAX_RETRY_WAIT=60  # Max wait between retries in seconds (default: 60s for vLLM recovery)

# ----------------------------------------------------------------------------
# Document Processing Configuration
# ----------------------------------------------------------------------------
# DOC_CHUNK_SIZE=800
# DOC_CHUNK_OVERLAP=80
# DOC_MAX_FILE_SIZE=300MB
# DOC_PRIMARY_PDF_PROCESSOR=mineru  # Options: mineru, traditional
# DOC_USE_TYPE_AWARE_CHUNKING=true

# ----------------------------------------------------------------------------
# MinerU OCR Configuration
# ----------------------------------------------------------------------------
# MINERU_API_TYPE=selfhosted  # Options: selfhosted, commercial
# MINERU_BASE_URL=http://192.168.0.136:8000
# MINERU_TIMEOUT=null
# MINERU_ENABLE_FORMULA=false
# MINERU_ENABLE_TABLE=true
# MINERU_LANGUAGE=en
# MINERU_MODEL_VERSION=pipeline  # Options: pipeline, vlm
# MINERU_SAVE_OUTPUTS=true
# MINERU_OUTPUT_DIR=/home/appuser/app/mineru_outputs
# MINERU_USE_ELEMENT_LEVEL_TYPES=true
# MINERU_ENABLE_ELEMENT_FILTERING=true

# MinerU Commercial API (only if api_type=commercial)
# MINERU_API_TOKEN=your_token_here
# MINERU_POLL_INTERVAL=10
# MINERU_MAX_RETRIES=3
# MINERU_SHARED_FOLDER_PATH=/shared/uploads
# MINERU_SHARED_FOLDER_URL_PREFIX=file:///shared/uploads

# ----------------------------------------------------------------------------
# Output Configuration
# ----------------------------------------------------------------------------
# OUTPUT_DEFAULT_FORMAT=json
# OUTPUT_DIRECTORY=./results
# OUTPUT_INCLUDE_METADATA=true
# OUTPUT_MAX_CONCURRENT_JOBS=5

# ----------------------------------------------------------------------------
# Path Configuration
# ----------------------------------------------------------------------------
# UPLOADS_DIR=/home/appuser/app/uploads
# PROMPTS_DIR=/home/appuser/app/prompts
# INPUT_DIR=/home/appuser/app/input
# OUTPUT_DIR=/home/appuser/app/output

# ----------------------------------------------------------------------------
# API Configuration
# ----------------------------------------------------------------------------
API_PORT=8000
# API_HOST=0.0.0.0
# API_RATE_LIMIT=9999

# API Authentication
# API_AUTH_ENABLED=false
# API_KEY=your_secret_api_key

# API Timeouts
# API_TIMEOUT_CONNECT=30
# API_TIMEOUT_READ=null

# CLI API Base URL (for local testing)
# FILEINTEL_API_BASE_URL=http://localhost:8000/api/v2

# ----------------------------------------------------------------------------
# Celery Configuration
# ----------------------------------------------------------------------------
# CELERY_TASK_SOFT_TIME_LIMIT=null
# CELERY_TASK_TIME_LIMIT=null
# CELERY_WORKER_POOL_RESTARTS=true
# CELERY_WORKER_POOL_RESTART_TIMEOUT=120

# Worker pool and concurrency configuration
# Pool types:
#   - solo: Single-threaded, no forking (low memory, limited RAM environments)
#   - prefork: Multi-process pool (high memory, scales with cores)
#   - threads: Multi-threaded (medium memory, I/O bound tasks)
#   - gevent: Async greenlets (low memory, async I/O)
# CELERY_WORKER_POOL=prefork  # Default: prefork for parallel processing
# CELERY_WORKER_CONCURRENCY=4  # Number of worker processes/threads

# Worker memory and task limits
# CELERY_WORKER_MAX_TASKS_PER_CHILD is now disabled (set to null in config/default.yaml)
# Workers never restart to prevent fork() SIGKILL issues - this is safe as application properly cleans up memory
# CELERY_WORKER_MEMORY_LIMIT=8G  # Docker memory hard limit
# CELERY_WORKER_MEMORY_RESERVATION=2G  # Guaranteed minimum memory
# CELERY_MEMORY_OVERHEAD_GB=1.0  # Memory overhead for Redis/DB pools

# Fork memory optimization (reduces OOM during worker creation)
# CELERYD_FORCE_EXECV=true  # Use exec() instead of fork() - creates fresh process (recommended for prefork)

# ----------------------------------------------------------------------------
# Gevent GraphRAG Worker Configuration (HIGH PERFORMANCE)
# ----------------------------------------------------------------------------
# Dedicated gevent worker for GraphRAG workloads with high concurrency
# Handles graphrag_indexing and graphrag_queries queues only

# CELERY_GRAPHRAG_CONCURRENCY=200  # Number of concurrent greenlets (default: 200)
# Conservative: 100, Balanced: 150, Aggressive: 200 (64GB RAM system)

# CELERY_GRAPHRAG_MEMORY_LIMIT=20G  # Docker memory hard limit (default: 20G for 64GB systems)
# Conservative: 12G, Balanced: 16G, Aggressive: 20G

# CELERY_GRAPHRAG_MEMORY_RESERVATION=2G  # Guaranteed minimum memory (default: 2G)

# Gevent-specific environment variables (automatically set in docker-compose.yml)
# GEVENT_RESOLVER=ares  # Async DNS resolver for non-blocking DNS lookups
# GEVENT_THREADPOOL_SIZE=50  # Thread pool for blocking operations
# PYTHONMALLOC=malloc  # Use system malloc (better for gevent)

# ----------------------------------------------------------------------------
# PostgreSQL Configuration (HIGH CONCURRENCY)
# ----------------------------------------------------------------------------
# Increase connection limits to support gevent concurrency
# Formula: gevent(200) + prefork(8×25) + api(50) + buffer(50) = ~500

# POSTGRES_MAX_CONNECTIONS=500  # Maximum concurrent connections (default: 500)
# Conservative: 300, Balanced: 400, Aggressive: 500

# POSTGRES_SHARED_BUFFERS=2GB  # PostgreSQL shared buffer cache (default: 2GB)
# POSTGRES_EFFECTIVE_CACHE_SIZE=6GB  # Query planner cache estimate (default: 6GB)
# POSTGRES_WORK_MEM=32MB  # Per-connection work memory (default: 32MB)

# POSTGRES_MEMORY_LIMIT=8G  # Docker memory limit for PostgreSQL (default: 8G)

# ----------------------------------------------------------------------------
# Redis Configuration (HIGH CONCURRENCY)
# ----------------------------------------------------------------------------
# REDIS_MAXMEMORY=2gb  # Maximum memory for Redis (default: 2gb)
# REDIS_MEMORY_LIMIT=3G  # Docker memory limit for Redis (default: 3G)

# ----------------------------------------------------------------------------
# CLI Configuration
# ----------------------------------------------------------------------------
# CLI_TASK_WAIT_TIMEOUT=null

# ----------------------------------------------------------------------------
# Batch Processing Configuration
# ----------------------------------------------------------------------------
# BATCH_DEFAULT_FORMAT=json
# BATCH_MAX_UPLOAD_SIZE=50
# BATCH_MAX_FILE_SIZE_MB=100
# BATCH_MAX_PROCESSING_SIZE=20
